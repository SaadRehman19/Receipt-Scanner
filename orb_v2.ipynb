{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from pyBEBLID import BEBLID_create\n",
    "\n",
    "# Read the input images in grayscale format (cv.IMREAD_GRAYSCALE)\n",
    "img1 = cv.imread('/home/gaditek/Receipt-Scanner/CustomerDataset/OPTP/Dollmen-Mall-Clifton/11798-638b75538e43b.png', 0)\n",
    "img2 = cv.imread('/home/gaditek/Receipt-Scanner/ReceiptDataset/OPTP/optp-dollmen-mall-clifton-karachi/4fOzaLl0WJWWZFB18m6g9tlzgreMh67BlKDywAaV.png', 0)\n",
    "\n",
    "# Create the feature detector, for example ORB\n",
    "detector = cv.ORB_create()\n",
    "\n",
    "# Detect features in both images\n",
    "points1 = detector.detect(img1, None)\n",
    "points2 = detector.detect(img2, None)\n",
    "print(\"Detected\", len(points1), \"kps in image1\")\n",
    "print(\"Detected\", len(points2), \"kps in image2\")\n",
    "\n",
    "# Use 32 bytes per descriptor and configure the scale factor for ORB detector\n",
    "descriptor = cv.BEBLID_create(256, 0.75)\n",
    "\n",
    "# Describe the detected features in both images\n",
    "_, descriptors1 = descriptor.compute(img1, points1)\n",
    "_, descriptors2 = descriptor.compute(img2, points2)\n",
    "print(\"Points described\")\n",
    "\n",
    "# Match the generated descriptors for img1 and img2 using brute force matching\n",
    "matcher = cv.BFMatcher_create(cv.NORM_HAMMING, crossCheck=True)\n",
    "matches = matcher.match(descriptors1, descriptors2)\n",
    "print(\"Number of matches:\", len(matches))\n",
    "\n",
    "# If there are not enough matches exit\n",
    "if len(matches) < 4:\n",
    "    exit(-1)\n",
    "\n",
    "# Take only the matched points that will be used to calculate the\n",
    "# transformation between both images\n",
    "matched_pts1 = np.array([points1[match.queryIdx].pt for match in matches])\n",
    "matched_pts2 = np.array([points2[match.trainIdx].pt for match in matches])\n",
    "\n",
    "# Find the homography that transforms a point in the first image to a point in the second image.\n",
    "H, inliers = cv.findHomography(matched_pts1, matched_pts2, cv.RANSAC, 3)\n",
    "# Print the number of inliers, that is, the number of points correctly\n",
    "# mapped by the transformation that we have estimated\n",
    "num_inliers = np.sum(inliers)\n",
    "print(\"Number of inliers:\", num_inliers, \"(\", 100.0 * num_inliers / len(matches), \"% )\")\n",
    "\n",
    "# Convert the images to BGR format from grayscale\n",
    "img1_bgr = cv.cvtColor(img1, cv.COLOR_GRAY2BGR)\n",
    "img2_bgr = cv.cvtColor(img2, cv.COLOR_GRAY2BGR)\n",
    "\n",
    "# Draw all the matched keypoints in red color\n",
    "all_matches_img = cv.drawMatches(img1_bgr, points1, img2_bgr, points2, matches, None, (0, 0, 255), (0, 0, 255))\n",
    "\n",
    "# Show and save the result\n",
    "cv.imshow(\"All matches\", all_matches_img)\n",
    "cv.waitKey()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
